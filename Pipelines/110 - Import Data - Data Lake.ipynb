{"cells":[{"cell_type":"markdown","source":["This example notebook closely follows the [Databricks documentation](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-datalake.html) for how to set up Azure Data Lake Store as a data source in Databricks."],"metadata":{}},{"cell_type":"markdown","source":["### 0 - Setup\n\nTo get set up, do these tasks first: \n\n- Get service credentials: Client ID `<aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee>` and Client Credential `<NzQzY2QzYTAtM2I3Zi00NzFmLWI3MGMtMzc4MzRjZmk=>`. Follow the instructions in [Create service principal with portal](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal). \n- Get directory ID `<ffffffff-gggg-hhhh-iiii-jjjjjjjjjjjj>`: This is also referred to as *tenant ID*. Follow the instructions in [Get tenant ID](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#get-tenant-id). \n- If you haven't set up the service app, follow this [tutorial](https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse). Set access at the root directory or desired folder level to the service or everyone."],"metadata":{}},{"cell_type":"markdown","source":["There are two options to read and write Azure Data Lake data from Azure Databricks:\n1. DBFS mount points\n2. Spark configs"],"metadata":{}},{"cell_type":"markdown","source":["## 1 - DBFS mount points\n[DBFS](https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html) mount points let you mount Azure Data Lake Store for all users in the workspace. Once it is mounted, the data can be accessed directly via a DBFS path from all clusters, without the need for providing credentials every time. The example below shows how to set up a mount point for Azure Data Lake Store."],"metadata":{}},{"cell_type":"code","source":["configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n           \"dfs.adls.oauth2.client.id\": \"2d3069bd****\",\n           # \"dfs.adls.oauth2.credential\": dbutils.secrets.get(scope = \"<scope-name>\", key = \"<key-name>\"),\n           \"dfs.adls.oauth2.credential\": \"t7ce/****\",\n           \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/d6397071****/oauth2/token\"}\n\n# Optionally, you can add <your-directory-name> to the source URI of your mount point.\ndbutils.fs.mount(\n  source = \"adl://devmsdata.azuredatalakestore.net/\", # что ?\n  mount_point = \"/mnt/data\", # куда внутри Databricks ?\n  extra_configs = configs) # используя какие параметры ?"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Verify that data source is mounted"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["%fs ls dbfs:/mnt/data/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/data/Raw Data/</td><td>Raw Data/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/Salaries.csv</td><td>Salaries.csv</td><td>16239776</td></tr><tr><td>dbfs:/mnt/data/sample_submission.csv</td><td>sample_submission.csv</td><td>343271</td></tr><tr><td>dbfs:/mnt/data/submission_0.csv/</td><td>submission_0.csv/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/submission_1.csv/</td><td>submission_1.csv/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/submission_2.csv/</td><td>submission_2.csv/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/submission_4.csv/</td><td>submission_4.csv/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/submission_5.csv/</td><td>submission_5.csv/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/submission_6.csv/</td><td>submission_6.csv/</td><td>0</td></tr><tr><td>dbfs:/mnt/data/test.csv</td><td>test.csv</td><td>983020</td></tr><tr><td>dbfs:/mnt/data/train.csv</td><td>train.csv</td><td>2516582400</td></tr></tbody></table></div>"]}}],"execution_count":7},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":8}],"metadata":{"name":"110 - Import Data - Data Lake","notebookId":1582041378450165},"nbformat":4,"nbformat_minor":0}